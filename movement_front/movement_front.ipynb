{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 18:02:45.722592: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-09 18:02:46.506873: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-09 18:02:48.088593: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-09 18:02:48.088708: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-09 18:02:48.326027: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-09 18:02:48.897504: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-09 18:02:48.899432: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-09 18:02:51.968641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/iotlabgpupc1/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.11):\n",
    "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "  Args:\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    height: height of the image in pixels.\n",
    "    width: width of the image in pixels.\n",
    "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "      visualized.\n",
    "\n",
    "  Returns:\n",
    "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "      * the coordinates of all keypoints of all detected entities;\n",
    "      * the coordinates of all skeleton edges of all detected entities;\n",
    "      * the colors in which the edges should be plotted.\n",
    "  \"\"\"\n",
    "  keypoints_all = []\n",
    "  keypoint_edges_all = []\n",
    "  edge_colors = []\n",
    "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "  for idx in range(num_instances):\n",
    "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "    kpts_absolute_xy = np.stack(\n",
    "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "        kpts_scores > keypoint_threshold, :]\n",
    "    keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "        keypoint_edges_all.append(line_seg)\n",
    "        edge_colors.append(color)\n",
    "  if keypoints_all:\n",
    "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "  else:\n",
    "    keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "  if keypoint_edges_all:\n",
    "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "  else:\n",
    "    edges_xy = np.zeros((0, 2, 2))\n",
    "  return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "  \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "  Args:\n",
    "    image: A numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "      of the crop region in normalized coordinates (see the init_crop_region\n",
    "      function below for more detail). If provided, this function will also\n",
    "      draw the bounding box on the image.\n",
    "    output_image_height: An integer indicating the height of the output image.\n",
    "      Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "  \"\"\"\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  # To remove the huge white borders\n",
    "  fig.tight_layout(pad=0)\n",
    "  ax.margins(0)\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_xticklabels([])\n",
    "  plt.axis('off')\n",
    "\n",
    "  im = ax.imshow(image)\n",
    "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "  ax.add_collection(line_segments)\n",
    "  # Turn off tick labels\n",
    "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "  (keypoint_locs, keypoint_edges,\n",
    "   edge_colors) = _keypoints_and_edges_for_display(\n",
    "       keypoints_with_scores, height, width)\n",
    "\n",
    "  line_segments.set_segments(keypoint_edges)\n",
    "  line_segments.set_color(edge_colors)\n",
    "  if keypoint_edges.shape[0]:\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "  if keypoint_locs.shape[0]:\n",
    "    scat.set_offsets(keypoint_locs)\n",
    "\n",
    "  if crop_region is not None:\n",
    "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "    rect = patches.Rectangle(\n",
    "        (xmin,ymin),rec_width,rec_height,\n",
    "        linewidth=1,edgecolor='b',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "  fig.canvas.draw()\n",
    "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "  image_from_plot = image_from_plot.reshape(\n",
    "      fig.canvas.get_width_height()[::-1] + (3,))\n",
    "  plt.close(fig)\n",
    "  if output_image_height is not None:\n",
    "    output_image_width = int(output_image_height / height * width)\n",
    "    image_from_plot = cv2.resize(\n",
    "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "         interpolation=cv2.INTER_CUBIC)\n",
    "  return image_from_plot\n",
    "\n",
    "def to_gif(images, duration):\n",
    "  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n",
    "  imageio.mimsave('./animation.gif', images, duration=duration)\n",
    "  return embed.embed_file('./animation.gif')\n",
    "\n",
    "def progress(value, max=100):\n",
    "  return HTML(\"\"\"\n",
    "      <progress\n",
    "          value='{value}'\n",
    "          max='{max}',\n",
    "          style='width: 100%'\n",
    "      >\n",
    "          {value}\n",
    "      </progress>\n",
    "  \"\"\".format(value=value, max=max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Loading the model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 18:03:05.138227: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-09 18:03:05.138454: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "model_name = \"movenet_thunder\"\n",
    "\n",
    "if \"movenet_lightning\" in model_name:\n",
    "  module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/2\")\n",
    "  input_size = 192\n",
    "elif \"movenet_thunder\" in model_name:\n",
    "  module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/2\")\n",
    "  input_size = 256\n",
    "else:\n",
    "  raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "def movenet(input_image):\n",
    "  \"\"\"Runs detection on an input image.\n",
    "\n",
    "  Args:\n",
    "    input_image: A [1, height, width, 3] tensor represents the input image\n",
    "      pixels. Note that the height/width should already be resized and match the\n",
    "      expected input resolution of the model before passing into this function.\n",
    "\n",
    "  Returns:\n",
    "    A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "    coordinates and scores.\n",
    "  \"\"\"\n",
    "  model = module.signatures['serving_default']\n",
    "\n",
    "  # SavedModel format expects tensor type of int32.\n",
    "  input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "  # Run model inference.\n",
    "  outputs = model(input_image)\n",
    "  # Output is a [1, 1, 17, 3] tensor.\n",
    "  keypoints_with_scores = outputs['output_0'].numpy()\n",
    "  return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Defining functions to calculate angles</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import atan\n",
    "\n",
    "def findAngle(point1, point2, point3):\n",
    "    M1 = (point1[1]-point2[1])/(point1[0]-point2[0])\n",
    "    M2 = (point3[1]-point2[1])/(point3[0]-point2[0])\n",
    "    PI = 3.14159265\n",
    "\t\n",
    "    angle = abs((M2 - M1) / (1 + M1 * M2))\n",
    "\n",
    "    ret = atan(angle)\n",
    "    return (round(ret, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "need to modify"
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_head_yaw(left_eye, right_eye, left_ear, right_ear, left_shoulder, right_shoulder):\n",
    "\n",
    "    midpoint_eyes = ((left_eye[0] + right_eye[0]) / 2, (left_eye[1] + right_eye[1]) / 2)\n",
    "    midpoint_ears = ((left_ear[0] + right_ear[0]) / 2, (left_ear[1] + right_ear[1]) / 2)\n",
    "    midpoint_shoulders = ((left_shoulder[0] + right_shoulder[0]) / 2, (left_shoulder[1] + right_shoulder[1]) / 2)\n",
    "\n",
    "    adjusted_midpoint_eyes = (midpoint_eyes[0] - midpoint_shoulders[0], midpoint_eyes[1] - midpoint_shoulders[1])\n",
    "    adjusted_midpoint_ears = (midpoint_ears[0] - midpoint_shoulders[0], midpoint_ears[1] - midpoint_shoulders[1])\n",
    "\n",
    "    delta_x = adjusted_midpoint_ears[0] - adjusted_midpoint_eyes[0]\n",
    "    delta_y = adjusted_midpoint_ears[1] - adjusted_midpoint_eyes[1]\n",
    "\n",
    "    angle = math.atan2(delta_y, delta_x)\n",
    "\n",
    "    return angle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Storing every angle in a JSON file to feed them to NAO</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture(0) \n",
    "  \n",
    "while(True):\n",
    "    ret, frame = vid.read() \n",
    "\n",
    "    image_data = frame\n",
    "    \n",
    "    image = tf.convert_to_tensor(image_data)\n",
    "    input_image = tf.expand_dims(image, axis=0)\n",
    "    input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "    keypoints_with_scores = movenet(input_image)\n",
    "\n",
    "    Lshoulder = keypoints_with_scores[0][0][5]\n",
    "    Lelbow = keypoints_with_scores[0][0][7]\n",
    "    Lwrist = keypoints_with_scores[0][0][9]\n",
    "    Lhip = keypoints_with_scores[0][0][11]\n",
    "    LAnkle = keypoints_with_scores[0][0][15]\n",
    "    LeftEar = keypoints_with_scores[0][0][3]\n",
    "    LKnee = keypoints_with_scores[0][0][13]\n",
    "    LeftEye = keypoints_with_scores[0][0][1]\n",
    "\n",
    "    RightEar = keypoints_with_scores[0][0][4]\n",
    "    Rshoulder = keypoints_with_scores[0][0][6]\n",
    "    Relbow = keypoints_with_scores[0][0][10]\n",
    "    Rwrist = keypoints_with_scores[0][0][9]\n",
    "    Rhip = keypoints_with_scores[0][0][12]\n",
    "    RAnkle = keypoints_with_scores[0][0][16]\n",
    "    RKnee = keypoints_with_scores[0][0][14]\n",
    "    RightEye = keypoints_with_scores[0][0][2]\n",
    "\n",
    "    head_angle = calculate_head_yaw(LeftEye , RightEye, LeftEar , RightEar, Lshoulder, Rshoulder)\n",
    "    Lshoulder_angle = findAngle(Lelbow, Lshoulder, Lhip)\n",
    "    Rshoulder_angle = findAngle(Relbow, Rshoulder, Rhip)\n",
    "\n",
    "    LElbow_angle= findAngle(Lshoulder , Lelbow , Lwrist)\n",
    "    RElbow_angle = findAngle(Rshoulder , Relbow , Rwrist)\n",
    "\n",
    "    data = {\n",
    "            \"RShoulderRoll\": -Rshoulder_angle , \n",
    "            \"LShoulderRoll\": Lshoulder_angle , \n",
    "            \"LElbowRoll\": -LElbow_angle, \n",
    "            \"RElbowRoll\": -RElbow_angle,\n",
    "            \"HeadYaw\":head_angle\n",
    "            }\n",
    "\n",
    "    # time.sleep(1)\n",
    "\n",
    "    with open('angles_front.json', 'w') as json_file:   \n",
    "        json.dump(data, json_file)\n",
    "\n",
    "\n",
    "    # display_image = tf.expand_dims(image, axis=0)\n",
    "    # display_image = tf.cast(tf.image.resize_with_pad(\n",
    "    #     display_image, 1280, 1280), dtype=tf.int32)\n",
    "    \n",
    "    # output_overlay = draw_prediction_on_image(\n",
    "    #     np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
    "    # cv2.imshow('output_overlay' ,output_overlay)\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #     break\n",
    "  \n",
    "vid.release() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
